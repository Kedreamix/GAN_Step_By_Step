# GAN Step By Step



![logo](https://img-blog.csdnimg.cn/dc199d960b704e0c9331376e069be96e.png#pic_center)

## å¿ƒè¡€æ¥æ½®

**[GSBS][1]**ï¼Œé¡¾åæ€ä¹‰ï¼Œæˆ‘å¸Œæœ›æˆ‘è‡ªå·±èƒ½å¤Ÿä¸€æ­¥ä¸€æ­¥çš„å­¦ä¹ GANã€‚GAN åˆå ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œæ˜¯æœ€è¿‘å‡ å¹´å¾ˆçƒ­é—¨çš„ä¸€ç§æ— ç›‘ç£ç®—æ³•ï¼Œä»–èƒ½ç”Ÿæˆå‡ºéå¸¸é€¼çœŸçš„ç…§ç‰‡ï¼Œå›¾åƒç”šè‡³è§†é¢‘ã€‚GANæ˜¯ä¸€ä¸ªå›¾åƒçš„å…¨æ–°çš„é¢†åŸŸï¼Œä»2014çš„GANçš„å‘å±•ç°åœ¨ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­æ‰®æ¼”è¿™è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œå¹¶ä¸”åˆ°æ¯å¹´éƒ½èƒ½äº§å‡ºå„è‰²å„æ ·çš„ä¸œè¥¿ï¼ŒGANçš„ç†è®ºå’Œå‘å±•éƒ½è›®å¤šçš„ã€‚æˆ‘æ„Ÿè§‰æœ€è¿‘æœ‰å¾ˆå¤šäººéƒ½åœ¨å­¦ä¹ GANï¼Œä½†æ˜¯å›½å†…å¯èƒ½ç¼ºå°‘æ¯”è¾ƒå¤šçš„GANçš„ç†è®ºåŠå…¶å®ç°ï¼Œæ‰€ä»¥æˆ‘ä¹Ÿæƒ³ç€å’Œå¤§å®¶ä¸€èµ·å­¦ä¹ ï¼Œå¹¶ä¸”æä¾›ä¸»æµæ¡†æ¶ä¸‹ **pytorch,tensorflow,keras** çš„ä¸€äº›å®ç°æ•™å­¦ã€‚

åœ¨ä¸€ä¸ª2016å¹´çš„ç ”è®¨ä¼šï¼Œ`æ¨ç«‹æ˜†`æè¿°ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œæ˜¯â€œ`æœºå™¨å­¦ä¹ è¿™äºŒåå¹´æ¥æœ€é…·çš„æƒ³æ³•`â€ã€‚

---

## Step5 ACGAN (Auxiliary Classifier GAN)

### Auxiliary Classifier GAN

*Auxiliary Classifier Generative Adversarial Network*

#### Authors

Augustus Odena, Christopher Olah, Jonathon Shlens

#### Abstract

Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.

[[Paper\]](https://arxiv.org/abs/1610.09585) [[Code\]][2]

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/37440b39108b4a8fb3bedcbd5a2bfefc.gif#pic_center)

**ACGAN**çš„å…¨ç§°å«Auxiliary Classifier Generative Adversarial Networkï¼Œç¿»è¯‘è¿‡æ¥å¾ˆç®€å•ï¼Œå°±æ˜¯å¸¦æœ‰è¾…åŠ©åˆ†ç±»å™¨çš„GAN

å…¶å®ä»–çš„æ€æƒ³å’ŒCGANå¾ˆæƒ³ï¼Œä¹Ÿæ˜¯åˆ©ç”¨labelçš„ä¿¡æ¯ä½œä¸ºå™ªå£°çš„è¾“å…¥çš„æ¡ä»¶æ¦‚ç‡ï¼Œä½†æ˜¯ç›¸æ¯”è¾ƒäºCGAN,ACGANåœ¨è®¾è®¡ä¸Šæ›´ä¸ºå·§å¦™ï¼Œä»–å¾ˆå¥½åœ°åˆ©ç”¨äº†åˆ¤åˆ«å™¨ä½¿å¾—ä¸ä½†å¯ä»¥åˆ¤åˆ«çœŸå‡ï¼Œä¹Ÿå¯ä»¥åˆ¤åˆ«ç±»åˆ«ï¼Œé€šè¿‡å¯¹ç”Ÿæˆå›¾åƒç±»åˆ«çš„åˆ¤æ–­ï¼Œåˆ¤åˆ«å™¨å¯ä»¥æ›´å¥½åœ°ä¼ é€’losså‡½æ•°ä½¿å¾—ç”Ÿæˆå™¨èƒ½å¤Ÿæ›´åŠ å‡†ç¡®åœ°æ‰¾åˆ°labelå¯¹åº”çš„å™ªå£°åˆ†å¸ƒã€‚

æˆ‘ä»¬ä¹Ÿå¯ä»¥ç®€å•çœ‹ä¸€ä¸‹GAN -> CGAN -> ACGANçš„ä¸€ä¸ªç»“æ„å˜åŒ–è¿‡ç¨‹

![GAN, conditional GAN (CGAN) and auxiliary classifier GAN (ACGAN)... |  Download Scientific Diagram](https://www.researchgate.net/publication/328494719/figure/fig1/AS:685463685853187@1540438686209/GAN-conditional-GAN-CGAN-and-auxiliary-classifier-GAN-ACGAN-architectures-where-x.ppm)



### CGANä¸ACGANçš„åŒºåˆ«

é€šè¿‡ä¸‹å›¾ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆæ¸…æ¥šçš„çœ‹åˆ°ï¼Œä¸CGANä¸åŒçš„æ˜¯ï¼Œ Cä¸ç›´æ¥è¾“å…¥ Dã€‚D ä¸ä»…éœ€è¦åˆ¤æ–­æ¯ä¸ªæ ·æœ¬çš„çœŸå‡ï¼Œè¿˜éœ€è¦å®Œæˆä¸€ä¸ªåˆ†ç±»ä»»åŠ¡å³é¢„æµ‹ C ï¼Œè¿™æ˜¯é€šè¿‡é€šè¿‡å¢åŠ ä¸€ä¸ªè¾…åŠ©åˆ†ç±»å™¨å®ç°ã€‚å¹¶ä¸”ï¼Œæˆ‘ä»¬çš„ACGANçš„åˆ¤åˆ«å™¨ä¸å†æ··å…¥labelï¼Œè€Œæ˜¯åœ¨é‰´åˆ«ç½‘ç»œçš„è¾“å‡ºæ—¶ï¼ŒæŠŠlabelä½œä¸ºtargetè¿›è¡Œåé¦ˆæ¥æäº¤ç»™é‰´åˆ«ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œå¯èƒ½è¿˜æœ‰ä¸€ä¸ªä¸åŒçš„ç‚¹ï¼Œåœ¨DCGANå‡ºç°ä¹‹åï¼Œåé¢çš„GANå‡ ä¹éƒ½ä½¿ç”¨äº†æ·±åº¦å·ç§¯ç½‘ç»œï¼Œå› ä¸ºå·ç§¯èƒ½å¤Ÿæ›´å¥½çš„æå–å›¾ç‰‡çš„ç‰¹å¾å€¼ï¼Œæ‰€æœ‰ACGANç”Ÿæˆçš„å›¾ç‰‡è¾¹ç¼˜æ›´å…·æœ‰è¿ç»­æ€§ï¼Œæ„Ÿè§‰æ›´çœŸå®ã€‚

![ACGAN | GitHub](https://ustccoder.github.io/images/Generative_adversarial/acgan.png)

å†ç»™å¤§å®¶ä¸¤å¹…å›¾æ¯”è¾ƒï¼Œè¿™æ ·å°±æ›´åŠ æ¸…æ¥šäº†

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/e973b4e7cdbc4647b5d97c9711f0bbea.png)

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/fab939c19547486c9d2c5a8c6b0c67ef.png)

ä¸Šé¢ç¬¬ä¸€å¼ æ˜¯ [CGAN][] çš„è®­ç»ƒæ¨¡å¼ï¼Œç¬¬äºŒå¼ æ˜¯ ACGAN çš„æ¨¡å¼ï¼Œä»–ä»¬çš„ä¸åŒç‚¹å…·ä½“ä½“ç°åœ¨ Discriminator ä¸Šï¼Œ

- CGAN çš„ Discriminator éœ€è¦åŒæ—¶æ¥æ”¶æ ‡ç­¾å’Œå›¾ç‰‡ä¿¡æ¯ï¼Œè¾“å‡ºæ˜¯å¦çœŸå®çš„ç»“è®º
- ACGAN çš„ Discriminator åªéœ€è¦æ¥æ”¶å›¾ç‰‡çš„è¾“å…¥ï¼Œè¿™ç‚¹å’Œ[ç»å…¸GAN][] ä¸€æ ·ï¼Œä½†æ˜¯éœ€è¦è¾“å‡ºå›¾ç‰‡çš„ç±»åˆ«å’Œæ˜¯å¦çœŸå®ä¸¤ä¸ªä»»åŠ¡

ä¸ºä»€ä¹ˆè¿™æ ·åšå‘¢ï¼Ÿå› ä¸ºä½œè€…çº¯ç²¹æ˜¯ä¸ºäº†è¾¾æˆå·æ‡’çš„ç›®çš„ï¼Œè¿™æ ·åšå¯ä»¥åŠ é€Ÿè®­ç»ƒã€‚ä½ é—®æˆ‘æ˜¯è¿™èƒ½åŠ é€Ÿå“ªï¼Ÿè¿™å¯å°± tricky äº†ï¼Œ å¦‚æœä½ è®© Discriminator è¾“å‡ºæ ‡ç­¾ï¼Œé‚£è¿™ä¸å°±æ˜¯ä¸€ä¸ªæ­£å¸¸ CNN è¯¥åšçš„äº‹å—ï¼Ÿè¿™å²‚ä¸æ˜¯å¯ä»¥ç›´æ¥æ‹¿ä¸€ä¸ªæ­£å¸¸ CNN æ¥åšè¿™ä»¶äº‹ï¼Ÿè€Œä¸”æ—¢ç„¶æ‹¿æ¥äº†ï¼Œ æˆ‘ä»¬æ˜¯ä¸æ˜¯å¯ä»¥æ‹¿ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„ CNNï¼Œæœ‰åˆ¤åˆ«èƒ½åŠ›çš„ CNNï¼Ÿå½“ç„¶å¯ä»¥ï¼Œè¿™ä¹ˆåšèƒ½å¤§å¤§å‡è½»æˆ‘ä»¬è®­ç»ƒçš„éš¾åº¦ï¼Œå› ä¸º Discriminator èƒ½æœ‰æ›´æœ‰èƒ½åŠ›å‘Šè¯‰ Generator ä½ è¿™ä¸ªç±»å‹ç”Ÿæˆå¾—å¯¹ä¸å¯¹äº†ã€‚ å½“ç„¶ï¼ŒDiscriminator çš„æœ¬è´¨å·¥ä½œ - åˆ¤æ–­æ˜¯å¦çœŸå®ï¼Œä¹Ÿæ˜¯è¿˜è¦ç»§ç»­åšçš„ã€‚

ä¸€å¥è¯æ¥ä»‹ç» ACGANï¼š **ä¸€ç§æŒ‰æ¡ä»¶ç”Ÿæˆå›¾ç‰‡çš„GANï¼Œå¹¶ä¸”ä½ è¿˜å¯ä»¥æ‹¿ç€é¢„è®­ç»ƒCNNæ¨¡å‹æ¥åŠ é€Ÿå®ƒçš„è®­ç»ƒè¿‡ç¨‹ã€‚**



### ACGANçš„æŸå¤±å‡½æ•°

é™¤æ­¤ä¹‹å¤–æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹æˆ‘ä»¬CGANçš„losså‡½æ•°:

![image-20190330012259653](https://tva4.sinaimg.cn/large/006tKfTcly1g1k6bmwocuj31e806qaco.jpg)

é‚£ä¹ˆåœ¨å¯¹æ¯”ä¸€ä¸‹æˆ‘ä»¬ACGANä»–çš„losså‡½æ•°:

![image-20190330012347134](https://tva3.sinaimg.cn/large/006tKfTcly1g1k6cgeiyij30xe06egnv.jpg)

$L_S$è¡¨ç¤ºçš„æ˜¯çœŸå®æ ·æœ¬å¯¹åº”ground truth,å‡çš„æ ·æœ¬å¯¹åº”fake.

$L_c$è¡¨ç¤ºçš„æ˜¯çœŸå®æ ·æœ¬å¯¹åº”ä»–çœŸå®çš„ç±»åˆ«ä¿¡æ¯,å‡çš„æ ·æœ¬å¯¹åº”çš„ä¹Ÿæ˜¯çœŸå®æ ·æœ¬çš„ç±»åˆ«ä¿¡æ¯



å…¶å®å¯ä»¥ç®€å•å°†å…¶ç†è§£ä¸º

$Ls$ï¼šå¯ä»¥ç†è§£ä¸ºåˆ¤åˆ«å™¨å°†â€œçœŸçš„â€åˆ¤åˆ«ä¸ºâ€œçœŸçš„â€ï¼Œâ€œå‡çš„â€åˆ¤åˆ«ä¸ºâ€œå‡çš„â€çš„èƒ½åŠ›

$Lc$ï¼šåˆ¤åˆ«å™¨å°†çœŸã€å‡æ•°æ®æ­£ç¡®åˆ†ç±»çš„èƒ½åŠ›



**åœ¨è®­ç»ƒåˆ¤åˆ«å™¨çš„æ—¶å€™,æˆ‘ä»¬å¸Œæœ›$L_S+L_C$æœ€å¤§åŒ–**

**åœ¨è®­ç»ƒç”Ÿæˆå™¨çš„æ—¶å€™,æˆ‘ä»¬å¸Œæœ›$L_C-L_S$æœ€å¤§åŒ–**

ç”Ÿæˆå™¨è¢«è®­ç»ƒä¸ºè¦ä½¿ $Lc-Ls$ æœ€å¤§åŒ–ï¼Œå³Gè¦ä½¿Lcæœ€å¤§åŒ–ï¼ŒLsæœ€å°åŒ–ï¼ŒLsä¸Lcä¸­å…³äºçœŸå®å›¾åƒçš„éƒ¨åˆ†ä¸Gæ— å…³ã€‚Lséƒ¨åˆ†ï¼ŒGè¦ä½¿å¾—å…¶ç”Ÿæˆçš„æ•°æ®è¢«åˆ¤åˆ«ä¸ºå‡çš„æ¦‚ç‡æœ€å°ï¼Œå³è¦ä½¿å¾—Gç”Ÿæˆçš„æ•°æ®æ›´é€¼çœŸï¼›Lcéƒ¨åˆ†ï¼ŒGè¦ä½¿å¾—å…¶ç”Ÿæˆæ•°æ®è¢«æ­£ç¡®åˆ†ç±»çš„æ¦‚ç‡æœ€å¤§ã€‚

åˆ¤åˆ«å™¨è¢«è®­ç»ƒä¸ºè¦ä½¿å¾— $Lc+Ls$ æœ€å¤§åŒ–ã€‚å³è¦ä½¿å¾—åˆ¤åˆ«å™¨é’ˆå¯¹çœŸå‡æ•°æ®ï¼Œåˆ†ç±»ã€åˆ¤åˆ«çš„èƒ½åŠ›éƒ½æœ€å¤§åŒ–.

## MNISTæ•°æ®é›†å®éªŒ

å…¶å®å’Œä¹‹å‰çš„CGAN,åªæ˜¯åšäº†å¾ˆå¾®å°çš„æ”¹å˜,ç”Ÿæˆå™¨å’ŒCGANç›¸æ¯”,å°±æ˜¯åŠ å…¥äº†å·ç§¯å±‚,ç›¸å½“äºæŠŠåŸæ¥CGANé‡Œé¢çš„å¤šå±‚æ„ŸçŸ¥æœºæ¢æˆäº†DCGANé‡Œé¢ä¸€æ ·çš„æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œ,é‚£ä¹ˆåˆ¤åˆ«å™¨åŒç†ï¼Œè¿™ä¹Ÿæ˜¯åœ¨DCGANå‡ºç°ä¹‹ååšçš„ã€‚



### åˆ¤åˆ«ç½‘ç»œ

é¦–å…ˆï¼Œæˆ‘ä»¬çš„åˆ¤åˆ«ç½‘ç»œæ˜¯ç”¨å·ç§¯ç½‘ç»œçš„ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œåˆ¤åˆ«å™¨çš„è¾“å‡ºæœ‰ä¸¤ä¸ª,ä¸€ä¸ªæ˜¯åˆ¤æ–­çœŸå‡çš„validity,ä¸€ä¸ªå›¾ç‰‡å¯¹åº”çš„labelä¿¡æ¯ï¼Œç›¸å½“äºåœ¨å·ç§¯å±‚çš„æœ«å°¾ç›¸å½“äºåšäº†ä¸€ä¸ªåˆ†å‰,ä¸€è¾¹æ˜¯åˆ¤æ–­çœŸå‡,ä¸€è¾¹æ˜¯è¿˜å¾—åˆ¤æ–­ç±»åˆ«.

```python
# åˆ¤åˆ«å™¨
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        def discriminator_block(in_filters, out_filters, bn=True):
            """Returns layers of each discriminator block"""
            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]
            if bn:
                block.append(nn.BatchNorm2d(out_filters, 0.8))
            return block
        # åˆ¤åˆ«ç½‘ç»œä¹Ÿåˆ©ç”¨å·ç§¯ç½‘è·¯
        self.conv_blocks = nn.Sequential(
            *discriminator_block(opt.channels, 16, bn=False),
            *discriminator_block(16, 32),
            *discriminator_block(32, 64),
            *discriminator_block(64, 128),
        )

        # The height and width of downsampled image
        ds_size = opt.img_size // 2 ** 4

        # Output layers
        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())
        # è¾…åŠ©åˆ†ç±»å±‚
        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())

    def forward(self, img):
        out = self.conv_blocks(img)
        out = out.view(out.shape[0], -1)
        validity = self.adv_layer(out)
        label = self.aux_layer(out)
        # åˆ¤åˆ«å™¨çš„è¾“å‡ºæœ‰ä¸¤ä¸ª,ä¸€ä¸ªæ˜¯åˆ¤æ–­çœŸå‡çš„validity,ä¸€ä¸ªå›¾ç‰‡å¯¹åº”çš„labelä¿¡æ¯
        return validity, label
```



### ç”Ÿæˆç½‘ç»œ

```python
# ACGAN ç”Ÿæˆå™¨
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()

        # å°†labelæ˜ å°„æˆäºzä¸€æ ·çš„ç»´åº¦
        self.label_emb = nn.Embedding(opt.n_classes, opt.latent_dim)

        self.init_size = opt.img_size // 4  # Initial size before upsampling
        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))

        # åˆ©ç”¨å·ç§¯ç½‘ç»œ
        self.conv_blocks = nn.Sequential(
            nn.BatchNorm2d(128),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 128, 3, stride=1, padding=1),
            nn.BatchNorm2d(128, 0.8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Upsample(scale_factor=2),
            nn.Conv2d(128, 64, 3, stride=1, padding=1),
            nn.BatchNorm2d(64, 0.8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),
            nn.Tanh(),
        )

    def forward(self, noise, labels):
        # ç›¸ä¹˜èµ·æ¥è¿™æ ·ä¾¿ä½¿å¾—noiseçš„è¾“å…¥æ˜¯å»ºç«‹åœ¨labelä½œä¸ºæ¡ä»¶çš„åŸºç¡€ä¸Š
        gen_input = torch.mul(self.label_emb(labels), noise)
        out = self.l1(gen_input)
        out = out.view(out.shape[0], 128, self.init_size, self.init_size)
        img = self.conv_blocks(out)
        return img
```



### è®­ç»ƒè¿‡ç¨‹

åˆ¤åˆ«å™¨ç›®æ ‡å‡½æ•°:
$$
\mathcal{L}^{(\mathrm{D})}=-\mathbb{E}_{\mathrm{x} \sim \text { pdata }} \log \mathrm{D}(\mathrm{x})-\mathbb{E}_{\mathrm{z}} \log [1-\mathrm{D}(\mathrm{G}(\mathrm{z} \mid \mathrm{y}))]-\mathbb{E}_{\mathrm{x} \sim \text { pdata }} \mathrm{p}(\mathrm{c} \mid \mathrm{x})-\mathbb{E}_{\mathrm{z}} \log \mathrm{p}(\mathrm{c} \mid \mathrm{g}(\mathrm{z} \mid \mathrm{y}))
$$
ç”Ÿæˆå™¨ç›®æ ‡å‡½æ•°ï¼š
$$
\mathcal{L}^{(G)}=-\mathbb{E}_{\mathrm{z}} \log \mathrm{D}(\mathrm{g}(\mathrm{z} \mid \mathrm{y}))-\mathbb{E}_{\mathrm{z}} \log \mathrm{p}(\mathrm{c} \mid \mathrm{g}(\mathrm{z} \mid \mathrm{y}))
$$

```python
# ----------
#  Training
# ----------

for epoch in range(opt.n_epochs):
    epoch_step = len(dataloader)
    with tqdm(total = epoch_step,desc = f'Epoch {epoch+1:3d}/{opt.n_epochs:3d}',postfix=dict,mininterval=0.3) as pbar:
        for i, (imgs, labels) in enumerate(dataloader):

            batch_size = imgs.shape[0]

            # Adversarial ground truths
            # å¾—åˆ°å¯¹æŠ—çš„ground truthsï¼Œvalidå…¨ä¸º1ï¼Œ fakeå…¨ä¸º0
            valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)
            fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)

            # Configure input è½¬åŒ–æ•°æ®æ ¼å¼
            real_imgs = Variable(imgs.type(FloatTensor))
            labels = Variable(labels.type(LongTensor))

            # Sample noise and labels as generator input
            # zä¸ºè®¾ç½®çš„å™ªå£°ï¼Œç”¨æ­£æ€åˆ†å¸ƒæ¥éšæœºç”Ÿæˆï¼Œå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
            z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))
            gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))

            # Generate a batch of images
            # ç”Ÿæˆä¸€ä¸ªbatchçš„å›¾ç‰‡ï¼Œå¹¶ä¸”ACGANåŠ å…¥ç”Ÿæˆçš„labels
            gen_imgs = generator(z, gen_labels)

            # ---------------------
            #  Train Discriminator è®­ç»ƒåˆ¤åˆ«å™¨
            # ---------------------

            optimizer_D.zero_grad()

            # Loss for real images
            real_pred, real_aux = discriminator(real_imgs)
            d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2

            # Loss for fake images
            fake_pred, fake_aux = discriminator(gen_imgs.detach())
            d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2

            # Total discriminator loss
            # ä¸ä»…éœ€è¦åˆ¤åˆ«ç±»åˆ«æ­£ç¡®ï¼Œå¹¶ä¸”ä¹Ÿéœ€è¦æ­£ç¡®åˆ¤å®šçœŸå‡
            d_loss = (d_real_loss + d_fake_loss) / 2  # -(LS + LC)

            # Calculate discriminator accuracy
            pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)
            gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)
            d_acc = np.mean(np.argmax(pred, axis=1) == gt)

            d_loss.backward() # è¿›è¡Œåå‘ä¼ æ’­
            optimizer_D.step() # æ›´æ–°å‚æ•°

            # -----------------
            #  Train Generator è®­ç»ƒç”Ÿæˆå™¨
            # -----------------

            optimizer_G.zero_grad()

            # Loss measures generator's ability to fool the discriminator
            validity, pred_label = discriminator(gen_imgs)
            # ä½¿å¾—ç”Ÿæˆå›¾ç‰‡çš„lossæœ€å°ï¼Œå¹¶ä¸”ç”Ÿæˆå›¾ç‰‡çš„labelä¸æŒ‡å®šçš„lossä¹Ÿæ˜¯æœ€å°çš„
            g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels)) 
            
            g_loss.backward() # è¿›è¡Œåå‘ä¼ æ’­
            optimizer_G.step() # æ›´æ–°å‚æ•°

            # æ¯sample_intervalä¸ªbatchsåä¿å­˜ä¸€æ¬¡imagesï¼Œå›¾ç‰‡æ”¾åœ¨imagesæ–‡ä»¶å¤¹ä¸‹
            batches_done = epoch * len(dataloader) + i
            if batches_done % opt.sample_interval == 0:
                sample_image(n_row=10, batches_done=batches_done)
            
            # åˆ©ç”¨tqdmå®æ—¶çš„å¾—åˆ°æˆ‘ä»¬çš„æŸå¤±çš„ç»“æœ
            pbar.set_postfix(**{'D Loss' : d_loss.item(),
                                'G Loss' : g_loss.item(),
                                'ACGAM Acc': '{:.4f}%'.format(100*d_acc.item())})
            pbar.update(1)
        # è¿›è¡Œtensorboardå¯è§†åŒ–ï¼Œ å¾—åˆ°çœŸå®çš„å›¾ç‰‡
        # ä»¥åŠè®°å½•å„ä¸ªå€¼ï¼Œè¿™æ ·æœ‰åŠ©äºæˆ‘ä»¬åˆ¤æ–­æŸå¤±çš„å˜åŒ–
        img_grid = make_grid(real_imgs)
        tbwriter.add_image(tag='real_image',img_tensor=img_grid,global_step=epoch+1)
        # img_grid = make_grid(gen_imgs)
        # tbwriter.add_image(tag='fake_image',img_tensor=img_grid,global_step=epoch+1)
        tbwriter.add_scalar('ACGAN_acc',d_acc.item(), global_step=epoch+1)
        tbwriter.add_scalar('dist_loss', d_loss.item(),global_step=epoch+1)
        tbwriter.add_scalar('gene_loss',g_loss.item(),global_step=epoch+1)
```



![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/cf6f01e47c9e4ffda44bf5aa06b2b0de.png)

è¿™é‡Œå¯ä»¥çœ‹åˆ°æœ€åçš„ç»“æœï¼Œå…¶å®ç”Ÿæˆçš„å›¾åƒå’ŒCGANæ˜¯å¾ˆåƒçš„ï¼Œä¸è¿‡å¯èƒ½å¤šäº†ä¸€ä¸ªåˆ†ç±»çš„ä¸€ä¸ªæ“ä½œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹ä¸€ä¸‹ï¼Œè®ºæ–‡ä¸­ç»™çš„å½©è‰²å›¾åƒçš„ç»“æœ

![img](https://pdf.cdn.readpaper.com/parsed/fetch_target/57bd38a9a33c13df6240de8e803060b8_6_Figure_9.png#pic_center)

æˆ‘ä»¬ä¼šå‘ç°ï¼Œç”Ÿæˆçš„å›¾åƒé©¬é©¬è™è™çš„ï¼Œå¹¶ä¸æ˜¯é‚£ä¹ˆçš„åè°ƒï¼Œä½†æ˜¯æ€»çš„æ¥è¯´ï¼Œè¿˜æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„çªç ´ï¼Œä¹‹åä¹Ÿæœ‰äººæå‡ºäº†ï¼Œå¦‚ä½•å»ç”Ÿæˆé«˜è´¨é‡çš„å›¾ç‰‡ï¼Œæˆ–è€…å¢åŠ å›¾ç‰‡çš„åˆ†è¾¨ç‡ã€‚éƒ½çœ‹åˆ°è¿™é‡Œäº†ï¼Œå¦‚æœæ„Ÿå…´è¶£çš„å°ä¼™ä¼´ä»¬ï¼Œè§‰å¾—è¿˜ä¸é”™çš„è¯ï¼Œå¯ä»¥ä¸‰è¿æ”¯æŒä¸€ä¸‹ï¼Œç‚¹èµ+è¯„è®º+æ”¶è—ï¼Œä½ ä»¬çš„æ”¯æŒå°±æ˜¯æˆ‘æœ€å¤§çš„åŠ¨åŠ›å•¦ï¼ğŸ˜„



[1]: https://redamancy.blog.csdn.net/article/details/127082815  "GSBS"
[2]: https://github.com/Dreaming-future/GAN_Step_By_Step/blob/main/Step5/acgan/acgan.py "acgan"
[3]: https://mofanpy.com/tutorials/machine-learning/gan/acgan "Auxiliary Classifier GAN (CGAN) åˆ†ç±»åŠ ç”Ÿæˆ"